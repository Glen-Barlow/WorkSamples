{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "buried-ordering",
   "metadata": {},
   "source": [
    "# Machine Learning Mini Project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-jewelry",
   "metadata": {},
   "source": [
    "### MSBA Seminar Spring 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-queens",
   "metadata": {},
   "source": [
    "### By Glen Barlow Via Nickolas Freeman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-fishing",
   "metadata": {},
   "source": [
    "## Step One: Read and prepare data for ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacterial-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "data_path = pathlib.Path('data', 'train.csv')\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "cols_to_consider = ['Geography','Gender']\n",
    "for col in cols_to_consider:\n",
    "    data = pd.concat([data, pd.get_dummies(data[col])], axis = 1)\n",
    "    data = data.drop(columns = [col])\n",
    "    \n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-longitude",
   "metadata": {},
   "source": [
    "## Step Two: Feature Engineering and Data Transformation for Machine Learning Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "patent-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Exited'\n",
    "features = [col for col in data.columns if col != target]\n",
    "data.loc[0,features].to_dict()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaled_data = scaler.fit(data[features])\n",
    "scaled_data = scaler.fit_transform(data[features])\n",
    "scaled_data = pd.DataFrame(scaled_data, columns = features)\n",
    "scaled_data[target] = data[target]\n",
    "\n",
    "scaler_means = {key: val for key, val in zip(features, scaler.mean_)}\n",
    "scaler_sigmas = {key: val for key, val in zip(features, scaler.scale_)}\n",
    "\n",
    "\n",
    "import json\n",
    "with open('scaler_means.json', 'w') as fout: \n",
    "    json.dump(scaler_means, fout)\n",
    "    \n",
    "with open('scaler_sigmas.json', 'w') as fout: \n",
    "    #json.dump(scaler_sigmas, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-vegetation",
   "metadata": {},
   "source": [
    "## Step Three: Fit and Save the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chronic-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(scaled_data, stratify = scaled_data[target], random_state = 0)\n",
    "x_train, y_train = train[features], train[target]\n",
    "x_test, y_test = test[features], test[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-furniture",
   "metadata": {},
   "source": [
    "### Gradient Boosted Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "international-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state = 0)\n",
    "\n",
    "clf = clf.fit(x_train, y_train)\n",
    "\n",
    "with open('gradientboost.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "    \n",
    "#with open('gradientboost.pkl', 'rb') as f:\n",
    "    #clf2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-spokesman",
   "metadata": {},
   "source": [
    "### AdaBoosted Decision Trees with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "novel-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_params = {\n",
    "    'learning_rate': 0.09000000000000001, \n",
    "    'n_estimators': 149,\n",
    "}\n",
    "\n",
    "clf = AdaBoostClassifier(random_state = 0, **adaboost_params)\n",
    "clf = clf.fit(x_train, y_train)\n",
    "with open('adaboost.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "    \n",
    "#with open('adaboost.pkl', 'rb') as f:\n",
    "#    clf2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-hawaiian",
   "metadata": {},
   "source": [
    "### Deep Neural Network With Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fitted-boutique",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', \n",
    "              optimizer = 'adam', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "X, y = train[features].values, train[target].values\n",
    "history = model.fit(X, y, \n",
    "          epochs = 95, \n",
    "          batch_size = 10, \n",
    "          verbose = 0,\n",
    "          validation_split = 0.2);\n",
    "\n",
    "with open('neural.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "    \n",
    "#with open('neural.pkl', 'rb') as f:\n",
    "#    clf2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-charlotte",
   "metadata": {},
   "source": [
    "### Combine to one Ensamble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "responces = GradientResponse + AdaBoostResponse + NeuralResponce  #Psuedo Code\n",
    "output = 0.5\n",
    "if responces >= 2:\n",
    "    output = 1\n",
    "else:\n",
    "    output = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-hometown",
   "metadata": {},
   "source": [
    "## Step 4: Deploy HTTP Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-infrastructure",
   "metadata": {},
   "source": [
    "## Step 5: Make Requests to Deployed HTTP Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "neural_url = <nueral enpoint>\n",
    "adaboost_url = <adaboost endpoint>\n",
    "gradient_url = <gradient url>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100\n",
    "data_dict = data.loc[index, features].to_dict()\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(neural_url, json = data_dict)\n",
    "nn_prediction = response.json()\n",
    "\n",
    "response = requests.post(adaboost_url, json = data_dict)\n",
    "ab_prediction = response.json()\n",
    "\n",
    "response = requests.post(gradient_url, json = data_dict)\n",
    "gb_prediction = response.json()\n",
    "\n",
    "responces = nn_prediction + ab_prediction + gb_prediction\n",
    "\n",
    "if responces >= 2:\n",
    "    output = 1\n",
    "else:\n",
    "    output = 0\n",
    "    \n",
    "print(f'{index}: NN -> {nn_prediction}, Ada -> {ab_prediction}, Grad -> {gb_prediction}')  \n",
    "print(f'{index}: Ensemble Response -> {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_comparison = {}\n",
    "for index in data.index:\n",
    "    if (index % 25) == 0:\n",
    "        print(f'Starting index {index}')\n",
    "    data_dict = data.loc[index, features].to_dict()\n",
    "\n",
    "    response = requests.post(neural_url, json = data_dict)\n",
    "    nn_prediction = response.json()\n",
    "\n",
    "    response = requests.post(adaboost_url, json = data_dict)\n",
    "    ab_prediction = response.json()\n",
    "    \n",
    "    response = requests.post(gradient_url, json = data_dict)\n",
    "    gb_prediction = response.json()\n",
    "    \n",
    "    prediction_comparison[index] = {\n",
    "        'NN': nn_prediction,\n",
    "        'AdaBoost': ab_prediction,\n",
    "        'GradBoost': gb_prediction,\n",
    "        'Ensemble': output,\n",
    "        'Actual': data.loc[index, target]\n",
    "    }\n",
    "\n",
    "prediction_comparison = pd.DataFrame().from_dict(prediction_comparison, orient = 'index')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "(prediction_comparison['NN'] == prediction_comparison['Actual']).sum()/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "(prediction_comparison['AdaBoost'] == prediction_comparison['Actual']).sum()/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "(prediction_comparison['GradBoost'] == prediction_comparison['Actual']).sum()/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "(prediction_comparison['Ensemble'] == prediction_comparison['Actual']).sum()/len(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
